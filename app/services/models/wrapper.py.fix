"""
Model Wrapper System for CasaLingua

This module provides wrapper classes for different model types,
standardizing how they're loaded, initialized, and used.
"""

import os
import logging
import torch
import asyncio
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from abc import ABC, abstractmethod

from app.utils.error_handler import APIError, ErrorCategory

# Configure logging
logger = logging.getLogger(__name__)

@dataclass
class ModelInput:
    """Data structure for model input"""
    text: Union[str, List[str]]
    source_language: str = "en"
    target_language: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None
    context: Optional[str] = None
    session_id: Optional[str] = None


@dataclass
class ModelOutput:
    """Data structure for model output"""
    result: Any
    metadata: Dict[str, Any] = field(default_factory=dict)


class BaseModelWrapper(ABC):
    """Base class for all model wrappers"""
    
    def __init__(self, model=None, tokenizer=None, config=None):
        """
        Initialize the model wrapper.
        
        Args:
            model: The underlying model
            tokenizer: The tokenizer for the model
            config: Configuration dictionary
        """
        self.model = model
        self.tokenizer = tokenizer
        self.config = config or {}
        
        # Default device is CPU
        self.device = self.config.get("device", "cpu")
        
        # Move model to device if it's a torch model
        if torch.cuda.is_available() and self.device == "cuda":
            logger.info(f"Using CUDA for model")
            # Make sure it's on the correct device
            if hasattr(model, "to") and callable(model.to):
                self.model = self.model.to(self.device)
        elif hasattr(torch, "mps") and torch.backends.mps.is_available() and self.device == "mps":
            logger.info(f"Using Apple MPS for model")
            # Make sure it's on the correct device
            if hasattr(model, "to") and callable(model.to):
                self.model = self.model.to(self.device)
    
    async def process(self, input_data: ModelInput) -> ModelOutput:
        """
        Process the input data using the model.
        
        Args:
            input_data: The input data
            
        Returns:
            The model output
        """
        try:
            # Preprocessing step
            preprocessed = self._preprocess(input_data)
            
            # Inference step
            model_output = await asyncio.to_thread(self._run_inference, preprocessed)
            
            # Postprocessing step
            output = self._postprocess(model_output, input_data)
            
            return output
        except Exception as e:
            logger.error(f"Error in model processing: {str(e)}", exc_info=True)
            # Re-raise as API error for proper handling
            raise APIError(
                status_code=500,
                error_code="model_processing_error",
                category=ErrorCategory.INTERNAL_ERROR,
                message=f"Error processing model: {str(e)}"
            )
    
    @abstractmethod
    def _preprocess(self, input_data: ModelInput) -> Dict[str, Any]:
        """
        Preprocess the input data for the model.
        
        Args:
            input_data: The input data
            
        Returns:
            The preprocessed data
        """
        pass
    
    @abstractmethod
    def _run_inference(self, preprocessed: Dict[str, Any]) -> Any:
        """
        Run inference with the model.
        
        Args:
            preprocessed: The preprocessed data
            
        Returns:
            The raw model output
        """
        pass
    
    @abstractmethod
    def _postprocess(self, model_output: Any, input_data: ModelInput) -> ModelOutput:
        """
        Postprocess the model output.
        
        Args:
            model_output: The raw model output
            input_data: The original input data
            
        Returns:
            The processed output
        """
        pass


class TranslationModelWrapper(BaseModelWrapper):
    """Wrapper for translation models"""
    
    def _preprocess(self, input_data: ModelInput) -> Dict[str, Any]:
        """Preprocess translation input"""
        if isinstance(input_data.text, list):
            texts = input_data.text
        else:
            texts = [input_data.text]
        
        # Get source and target languages
        source_lang = input_data.source_language
        target_lang = input_data.target_language
        
        # Handle missing target language
        if not target_lang:
            # Default to English if source is not English
            target_lang = "en" if source_lang != "en" else "es"
        
        # Handle MBART vs MT5 models
        model_name = getattr(getattr(self.model, "config", None), "_name_or_path", "") if hasattr(self.model, "config") else ""
        if "mbart" in model_name.lower():
            # MBART uses specific format
            source_lang_code = self._get_mbart_lang_code(source_lang)
            target_lang_code = self._get_mbart_lang_code(target_lang)
            
            # Tokenize inputs for MBART
            if self.tokenizer:
                inputs = self.tokenizer(
                    texts, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=self.config.get("max_length", 1024),
                    src_lang=source_lang_code
                )
                
                # Store target language for generation
                inputs["forced_bos_token_id"] = self.tokenizer.lang_code_to_id[target_lang_code]
                
                # Move to device
                for key in inputs:
                    if isinstance(inputs[key], torch.Tensor):
                        inputs[key] = inputs[key].to(self.device)
            else:
                inputs = {"texts": texts}
        else:
            # MT5 and other models use text prefix format
            prefixed_texts = [f"translate {source_lang} to {target_lang}: {text}" for text in texts]
            
            # Tokenize inputs
            if self.tokenizer:
                inputs = self.tokenizer(
                    prefixed_texts, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=self.config.get("max_length", 1024)
                )
                
                # Move to device
                for key in inputs:
                    if isinstance(inputs[key], torch.Tensor):
                        inputs[key] = inputs[key].to(self.device)
            else:
                inputs = {"texts": prefixed_texts}
        
        return {
            "inputs": inputs,
            "source_lang": source_lang,
            "target_lang": target_lang,
            "original_texts": texts
        }
    
    def _run_inference(self, preprocessed: Dict[str, Any]) -> Any:
        """Run translation inference"""
        inputs = preprocessed["inputs"]
        
        # Get generation parameters
        gen_kwargs = self.config.get("generation_kwargs", {}).copy()
        
        # Set defaults if not provided
        if "max_length" not in gen_kwargs:
            gen_kwargs["max_length"] = 512
        
        if "num_beams" not in gen_kwargs:
            gen_kwargs["num_beams"] = 4
        
        # Generate translations
        if hasattr(self.model, "generate") and callable(self.model.generate):
            return self.model.generate(
                **inputs,
                **gen_kwargs
            )
        elif hasattr(self.model, "translate") and callable(self.model.translate):
            # Direct translate method
            return self.model.translate(preprocessed["original_texts"])
        else:
            # Unknown model interface
            logger.error(f"Unsupported translation model: {type(self.model).__name__}")
            raise ValueError(f"Unsupported translation model: {type(self.model).__name__}")
    
    def _postprocess(self, model_output: Any, input_data: ModelInput) -> ModelOutput:
        """Postprocess translation output"""
        if not self.tokenizer:
            # Direct output mode
            return ModelOutput(
                result=model_output,
                metadata={"direct_output": True}
            )
        
        # Decode outputs
        translations = self.tokenizer.batch_decode(
            model_output, 
            skip_special_tokens=True
        )
        
        # Clean up outputs - remove any prefix that might have been generated
        prefixes_to_remove = [
            f"translate {input_data.source_language} to {input_data.target_language}:",
            f"{input_data.source_language} to {input_data.target_language}:",
            "translation:"
        ]
        
        cleaned_translations = []
        for translation in translations:
            for prefix in prefixes_to_remove:
                if translation.lower().startswith(prefix.lower()):
                    translation = translation[len(prefix):].strip()
            cleaned_translations.append(translation)
        
        # Return single or list result depending on input
        if isinstance(input_data.text, str):
            result = cleaned_translations[0] if cleaned_translations else ""
        else:
            result = cleaned_translations
        
        return ModelOutput(
            result=result,
            metadata={
                "source_language": input_data.source_language,
                "target_language": input_data.target_language
            }
        )
    
    def _get_mbart_lang_code(self, language_code: str) -> str:
        """Convert ISO language code to MBART language code format"""
        # MBART-50 uses language codes like "en_XX", "es_XX", etc.
        if language_code in ["zh", "zh-cn", "zh-CN"]:
            return "zh_CN"
        elif language_code in ["zh-tw", "zh-TW"]:
            return "zh_TW"
        else:
            # Just the base language code for most languages
            base_code = language_code.split("-")[0].lower()
            return f"{base_code}_XX"


class LanguageDetectionWrapper(BaseModelWrapper):
    """Wrapper for language detection models"""
    
    def _preprocess(self, input_data: ModelInput) -> Dict[str, Any]:
        """Preprocess language detection input"""
        if isinstance(input_data.text, list):
            texts = input_data.text
        else:
            texts = [input_data.text]
        
        # Tokenize inputs
        if self.tokenizer:
            inputs = self.tokenizer(
                texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=self.config.get("max_length", 512)
            )
            
            # Move to device
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor):
                    inputs[key] = inputs[key].to(self.device)
        else:
            inputs = {"texts": texts}
        
        return {
            "inputs": inputs,
            "original_texts": texts,
            "detailed": input_data.parameters.get("detailed", False) if input_data.parameters else False
        }
    
    def _run_inference(self, preprocessed: Dict[str, Any]) -> Any:
        """Run language detection inference"""
        inputs = preprocessed["inputs"]
        
        # Run directly if inputs is already processed for the model
        if hasattr(self.model, "forward") and callable(self.model.forward):
            with torch.no_grad():
                outputs = self.model(**inputs)
            return outputs
        elif hasattr(self.model, "detect_language") and callable(self.model.detect_language):
            # Direct detect_language method
            return self.model.detect_language(preprocessed["original_texts"])
        else:
            # Unknown model interface
            logger.error(f"Unsupported language detection model: {type(self.model).__name__}")
            raise ValueError(f"Unsupported language detection model: {type(self.model).__name__}")
    
    def _postprocess(self, model_output: Any, input_data: ModelInput) -> ModelOutput:
        """Postprocess language detection output"""
        detailed = input_data.parameters.get("detailed", False) if input_data.parameters else False
        
        # Direct output mode
        if not hasattr(model_output, "logits") and isinstance(model_output, (list, dict)):
            # Output already properly formatted
            return ModelOutput(
                result=model_output,
                metadata={"direct_output": True}
            )
        
        # Process logits
        logits = model_output.logits
        
        # Get language mappings - These depend on the model used
        id2label = getattr(self.model.config, "id2label", {})
        if not id2label:
            # Default language mappings for xlm-roberta model
            id2label = {
                0: "ar", 1: "bg", 2: "de", 3: "el", 4: "en", 5: "es", 
                6: "fr", 7: "hi", 8: "it", 9: "ja", 10: "nl", 
                11: "pl", 12: "pt", 13: "ru", 14: "sw", 15: "th", 
                16: "tr", 17: "ur", 18: "vi", 19: "zh"
            }
        
        # Convert logits to probabilities
        probs = torch.nn.functional.softmax(logits, dim=-1)
        
        # Process each input
        results = []
        for i in range(logits.shape[0]):
            # Get the top prediction
            values, indices = torch.topk(probs[i], k=5)
            
            # Convert to language codes and probabilities
            top_langs = [(id2label.get(idx.item(), f"lang_{idx.item()}"), val.item()) for idx, val in zip(indices, values)]
            
            # Format the result
            if detailed:
                # Detailed result with top 5 languages and probabilities
                result = {
                    "language": top_langs[0][0],
                    "confidence": top_langs[0][1],
                    "all_languages": {lang: prob for lang, prob in top_langs}
                }
            else:
                # Simple result with just the top language and confidence
                result = {
                    "language": top_langs[0][0],
                    "confidence": top_langs[0][1]
                }
            
            results.append(result)
        
        # Return single or list result depending on input
        if isinstance(input_data.text, str):
            result = results[0] if results else {"language": "unknown", "confidence": 0.0}
        else:
            result = results
        
        return ModelOutput(
            result=result,
            metadata={"detailed": detailed}
        )


class SimplificationModelWrapper(BaseModelWrapper):
    """Wrapper for text simplification models"""
    
    def _preprocess(self, input_data: ModelInput) -> Dict[str, Any]:
        """Preprocess simplification input"""
        if isinstance(input_data.text, list):
            texts = input_data.text
        else:
            texts = [input_data.text]
        
        # Get parameters
        parameters = input_data.parameters or {}
        
        # Get simplification level (1-5, where 5 is simplest)
        level = parameters.get("level", 3)
        grade_level = parameters.get("grade_level", None)
        
        # Map simplification level to grade level if not provided
        if grade_level is None:
            # Level 1 = grade 12, Level 5 = grade 4
            level_grade_map = {
                1: 12,  # College level
                2: 10,  # High school 
                3: 8,   # Middle school
                4: 6,   # Elementary school
                5: 4    # Early elementary
            }
            grade_level = level_grade_map.get(level, 8)
        
        # Check if we should use domain-specific simplification
        domain = parameters.get("domain", "")
        # Handle case where domain could be None
        is_legal_domain = domain and domain.lower() in ["legal", "housing", "housing_legal"]
        
        # Get model info
        model_config = getattr(self.model, "config", None)
        model_name = getattr(model_config, "_name_or_path", "") if model_config else ""
        model_class = self.model.__class__.__name__
        
        # Prepare custom prompts based on the level and model type
        prompts = []
        for text in texts:
            # Create prompt based on model type and simplification level
            if "bart" in model_name.lower() or "BartForConditionalGeneration" in model_class:
                # BART models with level-specific prompting
                if is_legal_domain:
                    # Legal domain with grade level
                    prompt = f"Simplify this housing legal text to a {grade_level}th grade reading level: {text}"
                else:
                    # Regular text with grade level
                    prompt = f"Simplify this text to a {grade_level}th grade reading level: {text}"
            elif "t5" in model_name.lower() or "T5ForConditionalGeneration" in model_class:
                # T5 models with level-specific prefixes
                if is_legal_domain:
                    # Legal simplification with grade level
                    prompt = f"simplify to grade {grade_level} legal text: {text}"
                else:
                    # Regular simplification with grade level
                    prompt = f"simplify to grade {grade_level}: {text}"
            else:
                # Generic model with basic prompting
                if is_legal_domain:
                    # Legal simplification with basic prompt
                    prompt = f"Simplify housing legal text (level {level}/5): {text}"
                else:
                    # Regular simplification with basic prompt
                    prompt = f"Simplify (level {level}/5): {text}"
            
            prompts.append(prompt)
        
        # Tokenize inputs
        if self.tokenizer:
            inputs = self.tokenizer(
                prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=self.config.get("max_length", 1024)
            )
            
            # Move to device
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor):
                    inputs[key] = inputs[key].to(self.device)
        else:
            inputs = {"texts": prompts}
        
        return {
            "inputs": inputs,
            "original_texts": texts,
            "level": level,
            "grade_level": grade_level,
            "domain": domain,
            "is_legal_domain": is_legal_domain,
            "prompts": prompts
        }
    
    def _run_inference(self, preprocessed: Dict[str, Any]) -> Any:
        """Run simplification inference with level-specific parameters"""
        inputs = preprocessed["inputs"]
        is_legal_domain = preprocessed.get("is_legal_domain", False)
        level = preprocessed.get("level", 3)
        
        # Get generation parameters with level-specific settings
        gen_kwargs = self.config.get("generation_kwargs", {}).copy()
        
        # Customize generation parameters based on simplification level
        # Level 1 (minimal simplification) -> Level 5 (maximum simplification)
        if "max_length" not in gen_kwargs:
            # Longer outputs for minimal simplification, shorter for maximum
            max_lengths = {
                1: 1024,  # Level 1: Longest outputs
                2: 896,
                3: 768, 
                4: 640,
                5: 512    # Level 5: Shortest outputs
            }
            gen_kwargs["max_length"] = max_lengths.get(level, 768)
        
        if "min_length" not in gen_kwargs:
            # Minimal simplification preserves more content
            min_lengths = {
                1: 100,   # Level 1: Preserve most content
                2: 80,
                3: 60,
                4: 40,
                5: 30     # Level 5: Aggressive simplification
            }
            # Adjust for legal domain which typically needs more content preserved
            if is_legal_domain:
                min_lengths = {k: v * 1.5 for k, v in min_lengths.items()}
            
            gen_kwargs["min_length"] = int(min_lengths.get(level, 60))
        
        if "num_beams" not in gen_kwargs:
            # More beams for higher quality at lower simplification levels
            beam_counts = {
                1: 8,     # Level 1: More careful generation
                2: 6,
                3: 5,
                4: 4,
                5: 4      # Level 5: Simpler generation
            }
            gen_kwargs["num_beams"] = beam_counts.get(level, 5)
        
        if "do_sample" not in gen_kwargs:
            # Sampling for more natural simplification
            gen_kwargs["do_sample"] = True
        
        if "temperature" not in gen_kwargs:
            # Temperature controls randomness - higher for more creative simplification
            temperatures = {
                1: 0.7,   # Level 1: More conservative
                2: 0.8,
                3: 0.9,
                4: 1.0,
                5: 1.1    # Level 5: More creative simplification
            }
            gen_kwargs["temperature"] = temperatures.get(level, 0.9)
        
        if "top_p" not in gen_kwargs:
            # Nucleus sampling threshold - smaller for more focused generation
            top_p_values = {
                1: 0.9,   # Level 1: More diverse outputs
                2: 0.85,
                3: 0.8,
                4: 0.75,
                5: 0.7    # Level 5: More focused on most likely tokens
            }
            gen_kwargs["top_p"] = top_p_values.get(level, 0.8)
        
        # Generate simplifications
        if hasattr(self.model, "generate") and callable(self.model.generate):
            return self.model.generate(
                **inputs,
                **gen_kwargs
            )
        elif hasattr(self.model, "simplify") and callable(self.model.simplify):
            # Direct simplify method
            return self.model.simplify(
                preprocessed["original_texts"],
                level=level
            )
        else:
            # Unknown model interface
            logger.error(f"Unsupported simplification model: {type(self.model).__name__}")
            raise ValueError(f"Unsupported simplification model: {type(self.model).__name__}")
    
    def _postprocess(self, model_output: Any, input_data: ModelInput) -> ModelOutput:
        """Postprocess simplification output with level-specific adjustments"""
        parameters = input_data.parameters or {}
        level = parameters.get("level", 3)
        grade_level = parameters.get("grade_level", None)
        domain = parameters.get("domain", "")
        is_legal_domain = domain and domain.lower() in ["legal", "housing", "housing_legal"]
        
        # Handle direct output format
        if not isinstance(model_output, torch.Tensor) and not self.tokenizer:
            return ModelOutput(
                result=model_output,
                metadata={
                    "level": level,
                    "grade_level": grade_level,
                    "domain": domain,
                    "direct_output": True
                }
            )
        
        # Decode outputs
        simplifications = self.tokenizer.batch_decode(
            model_output, 
            skip_special_tokens=True
        )
        
        # Clean up outputs - remove any prefix that might have been generated
        prefixes_to_remove = [
            f"Simplify this housing legal text to a {grade_level}th grade reading level:",
            f"Simplify this text to a {grade_level}th grade reading level:",
            f"simplify to grade {grade_level} legal text:",
            f"simplify to grade {grade_level}:",
            f"Simplify housing legal text (level {level}/5):",
            f"Simplify (level {level}/5):",
            "Simplify:",
            "Simplified version:",
            "Simplified text:"
        ]
        
        processed_simplifications = []
        for simplification in simplifications:
            # Remove prompt leftovers
            for prefix in prefixes_to_remove:
                if simplification.lower().startswith(prefix.lower()):
                    simplification = simplification[len(prefix):].strip()
            
            # Apply level-specific postprocessing
            if level >= 4:  # More aggressive for levels 4-5
                # Break long sentences for maximum simplification
                simplification = self._break_long_sentences(simplification)
                
                # Apply vocabulary substitutions for simple words
                simplification = self._apply_vocabulary_substitutions(simplification, level)
            
            # For legal domain, ensure proper formatting of legal terms
            if is_legal_domain:
                simplification = self._format_legal_terms(simplification)
            
            # Verify simplification actually happened
            if simplification.strip() == input_data.text.strip() if isinstance(input_data.text, str) else False:
                # No change detected, try rule-based simplification
                original_text = input_data.text if isinstance(input_data.text, str) else simplifications[0]
                simplification = self._rule_based_simplify(original_text, level)
            
            processed_simplifications.append(simplification)
        
        # Return single or list result depending on input
        if isinstance(input_data.text, str):
            result = processed_simplifications[0] if processed_simplifications else ""
        else:
            result = processed_simplifications
        
        return ModelOutput(
            result=result,
            metadata={
                "level": level,
                "grade_level": grade_level,
                "domain": domain
            }
        )
    
    def _break_long_sentences(self, text: str) -> str:
        """Break long sentences into shorter ones for better simplification"""
        import re
        
        # Split into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)
        simplified_sentences = []
        
        for sentence in sentences:
            # Skip already short sentences
            if len(sentence.split()) < 15:
                simplified_sentences.append(sentence)
                continue
            
            # Try to break at conjunctions
            parts = re.split(r', (?:and|but|or|because|however|moreover|furthermore|in addition)', sentence)
            
            if len(parts) > 1:
                for i, part in enumerate(parts):
                    if i > 0:
                        # Add appropriate connector for readability
                        connector = "Also, " if i > 0 else ""
                        simplified_sentences.append(connector + part.strip() + ".")
                    else:
                        simplified_sentences.append(part.strip() + ".")
            else:
                # If no conjunctions found, try to split at commas/semicolons
                parts = re.split(r'[;,] ', sentence)
                
                if len(parts) > 1:
                    for i, part in enumerate(parts):
                        # Add appropriate capitalization and endings
                        connector = "Also, " if i > 0 else ""
                        simplified_sentences.append(connector + part.strip() + ".")
                else:
                    # If no good split points, keep as is
                    simplified_sentences.append(sentence)
        
        # Join sentences
        result = " ".join(simplified_sentences)
        
        # Fix double periods
        result = re.sub(r'\.\.', '.', result)
        
        # Ensure proper capitalization
        result = re.sub(r'(?<=\. )[a-z]', lambda m: m.group(0).upper(), result)
        
        return result
    
    def _apply_vocabulary_substitutions(self, text: str, level: int) -> str:
        """Apply vocabulary substitutions based on simplification level"""
        import re
        
        # Define substitutions by level
        # Level 1: Minimal substitutions
        # Level 5: Maximum substitutions
        substitutions = {
            # Common for all levels
            1: {
                r'\butilize\b': 'use',
                r'\bpurchase\b': 'buy',
                r'\bsubsequently\b': 'later'
            },
            2: {
                r'\butilize\b': 'use',
                r'\bpurchase\b': 'buy',
                r'\bindicate\b': 'show',
                r'\bsufficient\b': 'enough',
                r'\bsubsequently\b': 'later',
                r'\badditional\b': 'more',
                r'\bprior to\b': 'before'
            },
            3: {
                r'\butilize\b': 'use',
                r'\bpurchase\b': 'buy',
                r'\bindicate\b': 'show',
                r'\bsufficient\b': 'enough',
                r'\bassist\b': 'help',
                r'\bobtain\b': 'get',
                r'\brequire\b': 'need',
                r'\badditional\b': 'more',
                r'\bprior to\b': 'before',
                r'\bsubsequently\b': 'later',
                r'\bcommence\b': 'start',
                r'\bterminate\b': 'end',
                r'\bdemonstrate\b': 'show'
            },
            4: {
                r'\butilize\b': 'use',
                r'\bpurchase\b': 'buy',
                r'\bindicate\b': 'show',
                r'\bsufficient\b': 'enough',
                r'\bassist\b': 'help',
                r'\bobtain\b': 'get',
                r'\brequire\b': 'need',
                r'\badditional\b': 'more',
                r'\bprior to\b': 'before',
                r'\bsubsequently\b': 'later',
                r'\bcommence\b': 'start',
                r'\bterminate\b': 'end',
                r'\bdemonstrate\b': 'show',
                r'\bregarding\b': 'about',
                r'\bimplement\b': 'use',
                r'\bnumerous\b': 'many',
                r'\bfacilitate\b': 'help',
                r'\binitial\b': 'first',
                r'\battempt\b': 'try',
                r'\bthus\b': 'so',
                r'\btherefore\b': 'so',
                r'\bconsequently\b': 'so',
                r'\bhence\b': 'so'
            },
            5: {
                r'\butilize\b': 'use',
                r'\bpurchase\b': 'buy',
                r'\bindicate\b': 'show',
                r'\bsufficient\b': 'enough',
                r'\bassist\b': 'help',
                r'\bobtain\b': 'get',
                r'\brequire\b': 'need',
                r'\badditional\b': 'more',
                r'\bprior to\b': 'before',
                r'\bsubsequently\b': 'later',
                r'\bcommence\b': 'start',
                r'\bterminate\b': 'end',
                r'\bdemonstrate\b': 'show',
                r'\bregarding\b': 'about',
                r'\bimplement\b': 'use',
                r'\bnumerous\b': 'many',
                r'\bfacilitate\b': 'help',
                r'\binitial\b': 'first',
                r'\battempt\b': 'try',
                r'\binquire\b': 'ask',
                r'\bascertain\b': 'find out',
                r'\bcomprehend\b': 'understand',
                r'\bnevertheless\b': 'however',
                r'\btherefore\b': 'so',
                r'\bfurthermore\b': 'also',
                r'\bconsequently\b': 'so',
                r'\bapproximately\b': 'about',
                r'\bmodification\b': 'change',
                r'\bendeavor\b': 'try',
                r'\bproficiency\b': 'skill',
                r'\bnecessitate\b': 'need',
                r'\bacquisition\b': 'getting',
                r'\bimmersion\b': 'practice',
                r'\bassimilation\b': 'learning',
                r'\bnotwithstanding\b': 'despite',
                r'\bmandatory\b': 'required',
                r'\bterminology\b': 'terms',
                r'\bexpedite\b': 'speed up',
                r'\beliminate\b': 'remove',
                r'\bprocure\b': 'get',
                r'\binitiate\b': 'start',
                r'\bconclude\b': 'end',
                r'\bcomponent\b': 'part',
                r'\bconsider\b': 'think about',
                r'\bdelay\b': 'wait',
                r'\bdifficult\b': 'hard',
                r'\beasy\b': 'simple',
                r'\bexplain\b': 'tell',
                r'\bfrequently\b': 'often',
                r'\bimportant\b': 'key',
                r'\bincrease\b': 'raise',
                r'\bdecrease\b': 'lower',
                r'\binform\b': 'tell',
                r'\blarger?\b': 'bigger',
                r'\bminimum\b': 'least',
                r'\bmaximum\b': 'most',
                r'\bnecessary\b': 'needed',
                r'\boptional\b': 'not needed',
                r'\bprovide\b': 'give',
                r'\brequire\b': 'need',
                r'\bselect\b': 'choose',
                r'\bverify\b': 'check'
            }
        }
        
        # Get the appropriate substitutions for this level and all lower levels
        all_substitutions = {}
        for l in range(1, level + 1):
            if l in substitutions:
                all_substitutions.update(substitutions[l])
        
        # Apply all substitutions
        result = text
        for pattern, replacement in all_substitutions.items():
            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)
        
        return result
    
    def _format_legal_terms(self, text: str) -> str:
        """Ensure proper formatting of legal terms"""
        import re
        
        # Dictionary of legal terms to properly format
        legal_terms = {
            r'\blandlord\b': 'Landlord',
            r'\btenant\b': 'Tenant',
            r'\blessor\b': 'Lessor',
            r'\blessee\b': 'Lessee',
            r'\bsecurity deposit\b': 'Security Deposit',
            r'\brent\b': 'Rent',
            r'\blease\b': 'Lease',
            r'\bagreement\b': 'Agreement',
            r'\bpremises\b': 'Premises',
            r'\bproperty\b': 'Property'
        }
        
        # Apply term formatting
        result = text
        for pattern, replacement in legal_terms.items():
            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)
        
        return result
    
    def _rule_based_simplify(self, text: str, level: int) -> str:
        """Apply rule-based simplification when model fails"""
        import re
        
        # If no text, return empty string
        if not text:
            return ""
        
        # Apply vocabulary substitutions based on level
        simplified = self._apply_vocabulary_substitutions(text, level)
        
        # For higher levels, break sentences
        if level >= 3:
            simplified = self._break_long_sentences(simplified)
        
        # Add explicit completion note based on level
        level_descriptions = {
            1: "",  # No explicit note for minimal simplification
            2: "simplified to level 2",
            3: "simplified for better understanding",
            4: "simplified for easier reading",
            5: "simplified to the simplest level"
        }
        
        # Check if we need to add a note
        if level > 1 and len(simplified) < len(text) * 0.9:
            # Only add the note if we've performed significant simplification
            note = level_descriptions.get(level, "")
            if note:
                if not simplified.strip().endswith("."):
                    simplified = simplified.strip() + "."
                simplified = simplified + " (" + note + ")"
        
        return simplified


class RAGGeneratorWrapper(BaseModelWrapper):
    """Wrapper for RAG generation models"""
    
    def _preprocess(self, input_data: ModelInput) -> Dict[str, Any]:
        """Preprocess RAG generation input"""
        if isinstance(input_data.text, list):
            query = input_data.text[0]  # Use the first item as the query
        else:
            query = input_data.text
        
        # Get context from parameters
        parameters = input_data.parameters or {}
        context = parameters.get("context", input_data.context or "")
        
        # Prepare prompt with context
        if context:
            prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
        else:
            prompt = f"Question: {query}\n\nAnswer:"
        
        # Tokenize input
        if self.tokenizer:
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=self.config.get("max_length", 1024)
            )
            
            # Move to device
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor):
                    inputs[key] = inputs[key].to(self.device)
        else:
            inputs = {"text": prompt}
        
        return {
            "inputs": inputs,
            "query": query,
            "context": context,
            "prompt": prompt
        }
    
    def _run_inference(self, preprocessed: Dict[str, Any]) -> Any:
        """Run RAG generation inference"""
        inputs = preprocessed["inputs"]
        
        # Get generation parameters
        gen_kwargs = self.config.get("generation_kwargs", {}).copy()
        
        # Set defaults if not provided
        if "max_length" not in gen_kwargs:
            gen_kwargs["max_length"] = 1024
        
        if "min_length" not in gen_kwargs:
            gen_kwargs["min_length"] = 50
        
        if "do_sample" not in gen_kwargs:
            gen_kwargs["do_sample"] = True
        
        if "temperature" not in gen_kwargs:
            gen_kwargs["temperature"] = 0.7
        
        if "top_p" not in gen_kwargs:
            gen_kwargs["top_p"] = 0.9
        
        if "top_k" not in gen_kwargs:
            gen_kwargs["top_k"] = 50
        
        if "num_beams" not in gen_kwargs:
            gen_kwargs["num_beams"] = 5
        
        # Generate response
        if hasattr(self.model, "generate") and callable(self.model.generate):
            return self.model.generate(
                **inputs,
                **gen_kwargs
            )
        elif hasattr(self.model, "generate_text") and callable(self.model.generate_text):
            # Direct generate_text method
            return self.model.generate_text(
                preprocessed["query"],
                context=preprocessed["context"]
            )
        else:
            # Unknown model interface
            logger.error(f"Unsupported RAG generation model: {type(self.model).__name__}")
            raise ValueError(f"Unsupported RAG generation model: {type(self.model).__name__}")
    
    def _postprocess(self, model_output: Any, input_data: ModelInput) -> ModelOutput:
        """Postprocess RAG generation output"""
        if not self.tokenizer:
            # Direct output mode
            return ModelOutput(
                result=model_output,
                metadata={"direct_output": True}
            )
        
        # Decode output
        response = self.tokenizer.decode(
            model_output[0], 
            skip_special_tokens=True
        )
        
        # Clean up output
        prefixes_to_remove = [
            "Context:",
            "Question:",
            "Answer:"
        ]
        
        for prefix in prefixes_to_remove:
            # Find the last occurrence of the prefix
            last_prefix_pos = response.rfind(prefix)
            if last_prefix_pos != -1:
                # Get everything after the prefix
                answer_start = last_prefix_pos + len(prefix)
                response = response[answer_start:].strip()
        
        return ModelOutput(
            result=response,
            metadata={
                "model_used": getattr(getattr(self.model, "config", None), "_name_or_path", "unknown") if hasattr(self.model, "config") else "unknown"
            }
        )


class RAGRetrieverWrapper(BaseModelWrapper):
    """Wrapper for RAG retrieval models"""
    
    def _preprocess(self, input_data: ModelInput) -> Dict[str, Any]:
        """Preprocess RAG retrieval input"""
        if isinstance(input_data.text, list):
            queries = input_data.text
        else:
            queries = [input_data.text]
        
        # Get parameters
        parameters = input_data.parameters or {}
        
        # Check if we should convert to embedding directly
        embed_only = parameters.get("embed_only", False)
        
        # Tokenize inputs
        if self.tokenizer and hasattr(self.model, "encode") and callable(self.model.encode):
            # Sentence transformer models handle tokenization differently
            inputs = {"texts": queries, "embed_only": embed_only}
        elif self.tokenizer:
            # Standard transformers tokenizer
            inputs = self.tokenizer(
                queries, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=self.config.get("max_length", 512)
            )
            
            # Move to device
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor):
                    inputs[key] = inputs[key].to(self.device)
        else:
            inputs = {"texts": queries, "embed_only": embed_only}
        
        return {
            "inputs": inputs,
            "queries": queries,
            "embed_only": embed_only,
            "parameters": parameters
        }
    
    def _run_inference(self, preprocessed: Dict[str, Any]) -> Any:
        """Run RAG retrieval inference"""
        inputs = preprocessed["inputs"]
        embed_only = preprocessed.get("embed_only", False)
        
        if hasattr(self.model, "encode") and callable(self.model.encode):
            # Sentence transformer encode method
            queries = inputs.get("texts", preprocessed["queries"])
            return self.model.encode(
                queries,
                convert_to_tensor=True,
                show_progress_bar=False
            )
        elif hasattr(self.model, "forward") and callable(self.model.forward):
            # Standard transformer forward pass
            with torch.no_grad():
                outputs = self.model(**inputs)
            return outputs
        else:
            # Unknown model interface
            logger.error(f"Unsupported RAG retrieval model: {type(self.model).__name__}")
            raise ValueError(f"Unsupported RAG retrieval model: {type(self.model).__name__}")
    
    def _postprocess(self, model_output: Any, input_data: ModelInput) -> ModelOutput:
        """Postprocess RAG retrieval output"""
        parameters = input_data.parameters or {}
        embed_only = parameters.get("embed_only", False)
        
        # Check if this is a sentence transformer embedding
        if isinstance(model_output, torch.Tensor) and model_output.dim() == 2:
            # Convert to list of lists
            embeddings = model_output.cpu().numpy().tolist()
            
            if embed_only:
                # Just return the embeddings
                if isinstance(input_data.text, list):
                    result = embeddings
                else:
                    result = embeddings[0] if embeddings else []
            else:
                # Return with metadata
                if isinstance(input_data.text, list):
                    result = {
                        "embeddings": embeddings,
                        "queries": input_data.text
                    }
                else:
                    result = {
                        "embedding": embeddings[0] if embeddings else [],
                        "query": input_data.text
                    }
            
            return ModelOutput(
                result=result,
                metadata={"embed_only": embed_only}
            )
        
        # Standard transformers output processing
        if hasattr(model_output, "pooler_output"):
            # Use pooler output for embeddings
            embeddings = model_output.pooler_output.cpu().numpy().tolist()
        elif hasattr(model_output, "last_hidden_state"):
            # Use mean of last hidden state for embeddings
            last_hidden = model_output.last_hidden_state
            mean_output = torch.mean(last_hidden, dim=1)
            embeddings = mean_output.cpu().numpy().tolist()
        else:
            # Unknown output format
            logger.error(f"Unknown output format for RAG retrieval: {type(model_output)}")
            embeddings = []
        
        if embed_only:
            # Just return the embeddings
            if isinstance(input_data.text, list):
                result = embeddings
            else:
                result = embeddings[0] if embeddings else []
        else:
            # Return with metadata
            if isinstance(input_data.text, list):
                result = {
                    "embeddings": embeddings,
                    "queries": input_data.text
                }
            else:
                result = {
                    "embedding": embeddings[0] if embeddings else [],
                    "query": input_data.text
                }
        
        return ModelOutput(
            result=result,
            metadata={"embed_only": embed_only}
        )


class AnonymizerWrapper(BaseModelWrapper):
    """Wrapper for text anonymization models"""
    
    def _preprocess(self, input_data: ModelInput) -> Dict[str, Any]:
        """Preprocess anonymization input"""
        if isinstance(input_data.text, list):
            texts = input_data.text
        else:
            texts = [input_data.text]
        
        # Get parameters
        parameters = input_data.parameters or {}
        
        # Determine entities to anonymize
        entities_to_anonymize = parameters.get("entities", ["PERSON", "LOCATION", "ORGANIZATION", "PHONE", "EMAIL", "SSN"])
        
        # Tokenize inputs
        if self.tokenizer:
            inputs = self.tokenizer(
                texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=self.config.get("max_length", 512)
            )
            
            # Move to device
            for key in inputs:
                if isinstance(inputs[key], torch.Tensor):
                    inputs[key] = inputs[key].to(self.device)
        else:
            inputs = {"texts": texts}
        
        return {
            "inputs": inputs,
            "original_texts": texts,
            "entities_to_anonymize": entities_to_anonymize
        }
    
    def _run_inference(self, preprocessed: Dict[str, Any]) -> Any:
        """Run anonymization inference"""
        inputs = preprocessed["inputs"]
        
        if hasattr(self.model, "forward") and callable(self.model.forward):
            # Standard NER model forward pass
            with torch.no_grad():
                outputs = self.model(**inputs)
            return outputs
        elif hasattr(self.model, "anonymize") and callable(self.model.anonymize):
            # Direct anonymize method
            return self.model.anonymize(
                preprocessed["original_texts"],
                entities_to_anonymize=preprocessed["entities_to_anonymize"]
            )
        else:
            # Unknown model interface
            logger.error(f"Unsupported anonymization model: {type(self.model).__name__}")
            raise ValueError(f"Unsupported anonymization model: {type(self.model).__name__}")
    
    def _postprocess(self, model_output: Any, input_data: ModelInput) -> ModelOutput:
        """Postprocess anonymization output"""
        parameters = input_data.parameters or {}
        entities_to_anonymize = parameters.get("entities", ["PERSON", "LOCATION", "ORGANIZATION", "PHONE", "EMAIL", "SSN"])
        replacement_style = parameters.get("replacement_style", "entity_type")  # Options: entity_type, generic, redacted
        
        # Direct output mode
        if isinstance(model_output, (list, dict)) and not hasattr(model_output, "logits"):
            return ModelOutput(
                result=model_output,
                metadata={"entities_anonymized": entities_to_anonymize}
            )
        
        # Process model outputs to get predicted entity tags
        if hasattr(model_output, "logits"):
            # Get predicted entity tags
            predictions = torch.argmax(model_output.logits, dim=2)
            
            # Get id2label mapping
            id2label = getattr(self.model.config, "id2label", {})
            
            # Get input token IDs
            input_ids = model_output.input_ids if hasattr(model_output, "input_ids") else None
            
            # Process each text
            anonymized_texts = []
            
            for i in range(predictions.shape[0]):
                tokens = self.tokenizer.convert_ids_to_tokens(input_ids[i]) if input_ids is not None else []
                predicted_labels = [id2label.get(t.item(), "O") for t in predictions[i]]
                
                # Create anonymized text
                anonymized_text = self._anonymize_text(tokens, predicted_labels, entities_to_anonymize, replacement_style)
                anonymized_texts.append(anonymized_text)
            
            # Return single or list result depending on input
            if isinstance(input_data.text, str):
                result = anonymized_texts[0] if anonymized_texts else ""
            else:
                result = anonymized_texts
            
            return ModelOutput(
                result=result,
                metadata={
                    "entities_anonymized": entities_to_anonymize,
                    "replacement_style": replacement_style
                }
            )
        else:
            # Unknown output format
            logger.error(f"Unknown output format for anonymization: {type(model_output)}")
            return ModelOutput(
                result=input_data.text,
                metadata={"error": "Unknown output format"}
            )
    
    def _anonymize_text(self, tokens, labels, entities_to_anonymize, replacement_style):
        """Anonymize text based on entity labels"""
        # Convert to BIO format if not already
        bio_labels = []
        for label in labels:
            if label == "O" or label.startswith("B-") or label.startswith("I-"):
                bio_labels.append(label)
            else:
                # Convert to BIO format
                bio_labels.append(f"B-{label}")
        
        # Process tokens and labels to identify entities
        anonymized_tokens = []
        i = 0
        while i < len(tokens):
            if bio_labels[i].startswith("B-"):
                # Start of an entity
                entity_type = bio_labels[i][2:]
                
                # Check if this entity should be anonymized
                if entity_type in entities_to_anonymize:
                    # Find the end of the entity
                    j = i + 1
                    while j < len(bio_labels) and bio_labels[j].startswith("I-") and bio_labels[j][2:] == entity_type:
                        j += 1
                    
                    # Generate replacement text based on style
                    if replacement_style == "entity_type":
                        replacement = f"[{entity_type}]"
                    elif replacement_style == "generic":
                        if entity_type == "PERSON":
                            replacement = "[NAME]"
                        elif entity_type == "LOCATION":
                            replacement = "[LOCATION]"
                        elif entity_type == "ORGANIZATION":
                            replacement = "[ORG]"
                        elif entity_type in ["PHONE", "EMAIL", "SSN"]:
                            replacement = "[CONTACT]"
                        else:
                            replacement = f"[{entity_type}]"
                    elif replacement_style == "redacted":
                        replacement = "[REDACTED]"
                    else:
                        replacement = f"[{entity_type}]"
                    
                    anonymized_tokens.append(replacement)
                    i = j  # Skip to the end of the entity
                else:
                    # Entity not to be anonymized
                    anonymized_tokens.append(tokens[i])
                    i += 1
            else:
                # Not an entity or continuation of entity
                anonymized_tokens.append(tokens[i])
                i += 1
        
        # Join tokens back into text
        # Handle special tokens and whitespace properly
        text = ""
        prev_token = ""
        
        for token in anonymized_tokens:
            if token.startswith("##"):
                # BERT-style continuation token
                text += token[2:]
            elif token.startswith("") or token.startswith(""):
                # RoBERTa/GPT style tokens with space prefix
                text += " " + token[1:]
            elif token in ["[CLS]", "[SEP]", "[PAD]", "<s>", "</s>", "<pad>"]:
                # Skip special tokens
                continue
            elif text and not (text.endswith(" ") or text.endswith("-") or prev_token.endswith("[")):
                # Add space between most tokens
                text += " " + token
            else:
                # First token or follows punctuation
                text += token
            
            prev_token = token
        
        return text.strip()


# Initialize wrapper_map with all the wrapper classes
wrapper_map = {
    "language_detection": LanguageDetectionWrapper,
    "translation": TranslationModelWrapper,
    "mbart_translation": TranslationModelWrapper,
    "mt5_translation": TranslationModelWrapper,
    "simplifier": SimplificationModelWrapper,
    "rag_generator": RAGGeneratorWrapper,
    "rag_retriever": RAGRetrieverWrapper,
    "anonymizer": AnonymizerWrapper
}

# Add any other wrapper classes as needed